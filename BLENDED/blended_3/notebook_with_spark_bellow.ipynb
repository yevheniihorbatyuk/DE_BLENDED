{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "import websockets\n",
    "import asyncio\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "\n",
    "class BinanceWebSocketProducer:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"Initialize the WebSocket producer with configuration\"\"\"\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "\n",
    "        self._setup_logging()\n",
    "        self._setup_kafka_producer()\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=self.config['logging']['level'],\n",
    "            format=self.config['logging']['format'],\n",
    "            filename=self.config['logging']['file']\n",
    "        )\n",
    "        self.logger = logging.getLogger('BinanceWebSocketProducer')\n",
    "\n",
    "        # Add console output\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(self.config['logging']['level'])\n",
    "        console_handler.setFormatter(logging.Formatter(self.config['logging']['format']))\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def _setup_kafka_producer(self):\n",
    "        \"\"\"Initialize Kafka producer with configuration\"\"\"\n",
    "        from configs import kafka_config\n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "            security_protocol=kafka_config['security_protocol'],\n",
    "            sasl_mechanism=kafka_config['sasl_mechanism'],\n",
    "            sasl_plain_username=kafka_config['username'],\n",
    "            sasl_plain_password=kafka_config['password'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            key_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        self.logger.info(\"Kafka producer initialized successfully\")\n",
    "\n",
    "    async def start_streaming(self):\n",
    "        \"\"\"Start WebSocket streaming\"\"\"\n",
    "        async with websockets.connect(self.config['binance']['websocket_url']) as websocket:\n",
    "            self.logger.info(\"Connected to Binance WebSocket\")\n",
    "\n",
    "            # Subscribe to streams\n",
    "            subscribe_message = {\n",
    "                \"method\": \"SUBSCRIBE\",\n",
    "                \"params\": [f\"{symbol}@trade\" for symbol in self.config['binance'].get('symbols', ['btcusdt'])],\n",
    "                \"id\": 1\n",
    "            }\n",
    "            await websocket.send(json.dumps(subscribe_message))\n",
    "            self.logger.info(f\"Sent subscription message: {subscribe_message}\")\n",
    "\n",
    "            while True:\n",
    "                message = await websocket.recv()\n",
    "                self.logger.info(f\"Received message: {message}\")\n",
    "\n",
    "                # Parse and send to Kafka\n",
    "                data = json.loads(message)\n",
    "                self.producer.send(\n",
    "                    self.config['topics']['raw_data'],\n",
    "                    value=data,\n",
    "                    key=str(uuid.uuid4())\n",
    "                )\n",
    "                self.producer.flush()\n",
    "                self.logger.info(f\"Message sent to Kafka: {data}\")\n",
    "\n",
    "    def shutdown(self):\n",
    "        \"\"\"Clean shutdown of the producer\"\"\"\n",
    "        try:\n",
    "            self.producer.flush()\n",
    "            self.producer.close()\n",
    "            self.logger.info(\"Producer shut down successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during shutdown: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    producer = BinanceWebSocketProducer('config.yaml')\n",
    "\n",
    "    async def run_producer():\n",
    "        try:\n",
    "            await producer.start_streaming()\n",
    "        except KeyboardInterrupt:\n",
    "            producer.logger.info(\"Received shutdown signal\")\n",
    "        finally:\n",
    "            producer.shutdown()\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # Для середовищ, де цикл вже працює\n",
    "        loop.create_task(run_producer())\n",
    "    else:\n",
    "        loop.run_until_complete(run_producer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = BinanceWebSocketProducer('./config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.config['binance'].get('symbols', 'btcusdt@trade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer._setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self._setup_logging()\n",
    "consumer._setup_kafka_consumer()\n",
    "# self._setup_kafka_producer()\n",
    "# self._setup_symbol_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer._setup_kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    await producer.start_streaming()\n",
    "except KeyboardInterrupt:\n",
    "    producer.logger.info(\"Received shutdown signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def setup_logging(config: Dict[str, Any]) -> logging.Logger:\n",
    "    \"\"\"Configure and return logger\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=config['logging']['level'],\n",
    "        format=config['logging']['format'],\n",
    "        filename=config['logging']['file']\n",
    "    )\n",
    "    logger = logging.getLogger('ForexKafkaConsumer')\n",
    "    return logger\n",
    "\n",
    "\n",
    "def setup_kafka_consumer(config: Dict[str, Any], logger: logging.Logger) -> KafkaConsumer:\n",
    "    \"\"\"Initialize Kafka consumer\"\"\"\n",
    "    try:\n",
    "        consumer = KafkaConsumer(\n",
    "            config['topics']['raw_data'],\n",
    "            bootstrap_servers=config['kafka']['bootstrap_servers'],\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            **config['kafka']['consumer_config']\n",
    "        )\n",
    "        logger.info(\"Kafka consumer initialized successfully\")\n",
    "        return consumer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Kafka consumer: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def setup_kafka_producer(config: Dict[str, Any], logger: logging.Logger) -> KafkaProducer:\n",
    "    \"\"\"Initialize Kafka producer for filtered streams\"\"\"\n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=config['kafka']['bootstrap_servers'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            **config['kafka']['producer_config']\n",
    "        )\n",
    "        logger.info(\"Kafka producer initialized successfully\")\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Kafka producer: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def setup_symbol_mapping(config: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Create mapping of symbols to their respective topics\"\"\"\n",
    "    return {\n",
    "        pair['symbol'].lower(): pair['topic']\n",
    "        for pair in config['topics']['currency_pairs']\n",
    "    }\n",
    "\n",
    "\n",
    "def process_message(\n",
    "    message: Dict[str, Any],\n",
    "    symbol_topics: Dict[str, str],\n",
    "    producer: KafkaProducer,\n",
    "    logger: logging.Logger\n",
    "):\n",
    "    \"\"\"Process individual message and produce to appropriate topic\"\"\"\n",
    "    try:\n",
    "        symbol = message.get('s', '').lower()\n",
    "\n",
    "        if symbol in symbol_topics:\n",
    "            # Transform message for downstream processing\n",
    "            processed_data = {\n",
    "                'symbol': symbol,\n",
    "                'price': float(message.get('p', 0)),\n",
    "                'quantity': float(message.get('q', 0)),\n",
    "                'timestamp': message.get('T', 0),\n",
    "                'processing_timestamp': message.get('processing_timestamp')\n",
    "            }\n",
    "\n",
    "            # Produce to symbol-specific topic\n",
    "            producer.send(\n",
    "                symbol_topics[symbol],\n",
    "                value=processed_data\n",
    "            )\n",
    "            logger.debug(f\"Processed message for symbol {symbol}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing message: {str(e)}\")\n",
    "\n",
    "\n",
    "def start_consuming(config_path: str):\n",
    "    \"\"\"Main logic for consuming and processing messages\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    logger = setup_logging(config)\n",
    "    consumer = setup_kafka_consumer(config, logger)\n",
    "    producer = setup_kafka_producer(config, logger)\n",
    "    symbol_topics = setup_symbol_mapping(config)\n",
    "\n",
    "    logger.info(\"Starting to consume messages\")\n",
    "    try:\n",
    "        for msg in consumer:\n",
    "            process_message(msg.value, symbol_topics, producer, logger)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received shutdown signal\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\")\n",
    "    finally:\n",
    "        shutdown(consumer, producer, logger)\n",
    "\n",
    "\n",
    "def shutdown(consumer: KafkaConsumer, producer: KafkaProducer, logger: logging.Logger):\n",
    "    \"\"\"Clean shutdown of consumer and producer\"\"\"\n",
    "    try:\n",
    "        consumer.close()\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "        logger.info(\"Consumer and producer shut down successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during shutdown: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_consuming('config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from configs import kafka_config\n",
    "import json\n",
    "\n",
    "def safe_deserialize(data):\n",
    "    return json.loads(data.decode('utf-8')) if data else None\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "    security_protocol=kafka_config['security_protocol'],\n",
    "    sasl_mechanism=kafka_config['sasl_mechanism'],\n",
    "    sasl_plain_username=kafka_config['username'],\n",
    "    sasl_plain_password=kafka_config['password'],\n",
    "    value_deserializer=safe_deserialize,\n",
    "    key_deserializer=safe_deserialize,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my_consumer_group_1'\n",
    ")\n",
    "\n",
    "topic_name = \"currency_raw_data\"\n",
    "\n",
    "consumer.subscribe([topic_name])\n",
    "\n",
    "print(f\"Subscribed to topic '{topic_name}'\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        print(f\"Raw Kafka message: {message}\")\n",
    "        print(f\"Message key: {message.key}, Message value: {message.value}\")\n",
    "\n",
    "        data = message.value\n",
    "        if data:\n",
    "            print(f\"Deserialized value: {data}\")\n",
    "        else:\n",
    "            print(\"Message value is None or could not be deserialized.\")\n",
    "\n",
    "        # Ваші додаткові обробки тут\n",
    "        # time.sleep(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from configs import kafka_config\n",
    "import os\n",
    "\n",
    "os.environ[\n",
    "    'PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaStreaming\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.debug.maxToStringFields\", \"200\")\n",
    "         .config(\"spark.sql.columnNameLengthThreshold\", \"200\")\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    window, avg, min, max, stddev, count,\n",
    "    percentile_approx, col, from_json, struct\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType,\n",
    "    DoubleType, LongType, TimestampType\n",
    ")\n",
    "\n",
    "class ForexSparkStreamProcessor:\n",
    "    \"\"\"\n",
    "    Spark Streaming processor for aggregating forex data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"Initialize Spark Streaming processor with configuration\"\"\"\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        \n",
    "        self._setup_logging()\n",
    "        self._setup_spark()\n",
    "        self._setup_schema()\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=self.config['logging']['level'],\n",
    "            format=self.config['logging']['format'],\n",
    "            filename=self.config['logging']['file']\n",
    "        )\n",
    "        self.logger = logging.getLogger('ForexSparkStreamProcessor')\n",
    "    \n",
    "    def _setup_spark(self):\n",
    "        \"\"\"Initialize Spark session\"\"\"\n",
    "        try:\n",
    "            self.spark = (SparkSession.builder\n",
    "                .appName(self.config['spark']['app_name'])\n",
    "                .master(self.config['spark']['master'])\n",
    "                .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "                .getOrCreate())\n",
    "            \n",
    "            self.logger.info(\"Spark session initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize Spark session: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_schema(self):\n",
    "        \"\"\"Define schema for incoming data\"\"\"\n",
    "        self.schema = StructType([\n",
    "            StructField(\"symbol\", StringType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"quantity\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True),\n",
    "            StructField(\"processing_timestamp\", StringType(), True)\n",
    "        ])\n",
    "    \n",
    "    def _create_streaming_query(self, topic: str):\n",
    "        \"\"\"Create and return streaming query for given topic\"\"\"\n",
    "        return (self.spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\",\n",
    "                   self.config['kafka']['bootstrap_servers'])\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"startingOffsets\", \"latest\")\n",
    "            .load()\n",
    "            .select(from_json(\n",
    "                col(\"value\").cast(\"string\"),\n",
    "                self.schema\n",
    "            ).alias(\"data\"))\n",
    "            .select(\"data.*\"))\n",
    "    \n",
    "    def _process_stream(self, df, topic: str):\n",
    "        \"\"\"Process streaming dataframe with windowed aggregations\"\"\"\n",
    "        window_duration = self.config['spark']['window_duration']\n",
    "        slide_duration = self.config['spark']['slide_duration']\n",
    "        \n",
    "        # Calculate aggregations\n",
    "        aggregations = (df\n",
    "            .withWatermark(\"timestamp\", window_duration)\n",
    "            .groupBy(window(col(\"timestamp\"), window_duration, slide_duration))\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                min(\"price\").alias(\"min_price\"),\n",
    "                max(\"price\").alias(\"max_price\"),\n",
    "                stddev(\"price\").alias(\"price_stddev\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                percentile_approx(\"price\", array(0.25, 0.5, 0.75), 100)\n",
    "                    .alias(\"price_percentiles\"),\n",
    "                avg(\"quantity\").alias(\"avg_quantity\"),\n",
    "                sum(\"quantity\").alias(\"total_volume\")\n",
    "            ))\n",
    "        \n",
    "        # Write results to console (modify for production)\n",
    "        query = (aggregations.writeStream\n",
    "            .outputMode(\"complete\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\", \"false\")\n",
    "            .option(\"numRows\", 100)\n",
    "            .trigger(processingTime=slide_duration)\n",
    "            .start())\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def start_processing(self, topic: str):\n",
    "        \"\"\"Start processing streams for given topic\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting stream processing for topic: {topic}\")\n",
    "            \n",
    "            # Create streaming dataframe\n",
    "            streaming_df = self._create_streaming_query(topic)\n",
    "            \n",
    "            # Process stream and get query\n",
    "            query = self._process_stream(streaming_df, topic)\n",
    "            \n",
    "            # Wait for query termination\n",
    "            query.awaitTermination()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in stream processing: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.shutdown()\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Clean shutdown of Spark session\"\"\"\n",
    "        try:\n",
    "            self.spark.stop()\n",
    "            self.logger.info(\"Spark session shut down successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during shutdown: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for Spark Streaming processor\"\"\"\n",
    "    processor = ForexSparkStreamProcessor('config.yaml')\n",
    "    \n",
    "    # Process BTCUSDT stream as example\n",
    "    topic = 'forex_btcusdt'\n",
    "    processor.start_processing(topic)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, LongType, BooleanType\n",
    ")\n",
    "from configs import kafka_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Налаштування PySpark для використання Kafka\n",
    "os.environ[\n",
    "    'PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ініціалізація SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"ForexSparkStreamProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.debug.maxToStringFields\", \"200\")\n",
    "         .config(\"spark.sql.columnNameLengthThreshold\", \"200\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.executor.cores\", \"4\")\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Налаштування параметрів вікна\n",
    "window_duration = \"1 minute\"\n",
    "sliding_interval = \"30 seconds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Схема вхідних даних\n",
    "schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", LongType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"trade_id\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"trade_time\", LongType(), True),\n",
    "    StructField(\"buyer_market_maker\", BooleanType(), True),\n",
    "    StructField(\"ignore\", BooleanType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Зчитування потоку з Kafka\n",
    "df = (spark.readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", kafka_config['bootstrap_servers'][0])\n",
    "      .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\")\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "      .option(\"kafka.sasl.jaas.config\",\n",
    "              'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"VawEzo1ikLtrA8Ug8THa\";')\n",
    "      .option(\"subscribe\", 'currency_raw_data')\n",
    "      .option(\"startingOffsets\", \"latest\")\n",
    "      .option(\"maxOffsetsPerTrigger\", \"500\")  # Обмеження кількості даних на тригер\n",
    "      .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Обробка даних\n",
    "processed_df = (df\n",
    "                .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "                .select(\n",
    "                    col(\"data.symbol\"),\n",
    "                    col(\"data.price\"),\n",
    "                    col(\"data.quantity\"),\n",
    "                    col(\"data.trade_time\").alias(\"timestamp\")\n",
    "                )\n",
    "                .withColumn(\"event_time\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "                .groupBy(window(col(\"event_time\"), window_duration, sliding_interval))\n",
    "                .agg(\n",
    "                    avg(\"price\").alias(\"avg_price\"),\n",
    "                    min(\"price\").alias(\"min_price\"),\n",
    "                    max(\"price\").alias(\"max_price\"),\n",
    "                    stddev(\"price\").alias(\"price_stddev\"),\n",
    "                    count(\"*\").alias(\"trade_count\"),\n",
    "                    avg(\"quantity\").alias(\"avg_quantity\"),\n",
    "                    sum(\"quantity\").alias(\"total_volume\"),\n",
    "                    percentile_approx(\"price\", array(lit(0.25), lit(0.5), lit(0.75)), 100).alias(\"price_percentiles\")\n",
    "                ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Додавання ключа та підготовка до запису\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "output_df = (processed_df\n",
    "             .withColumn(\"key\", uuid_udf())\n",
    "             .select(\n",
    "                 col(\"key\"),\n",
    "                 to_json(struct(\n",
    "                     col(\"window\"),\n",
    "                     col(\"avg_price\"),\n",
    "                     col(\"min_price\"),\n",
    "                     col(\"max_price\"),\n",
    "                     col(\"price_stddev\"),\n",
    "                     col(\"trade_count\"),\n",
    "                     col(\"avg_quantity\"),\n",
    "                     col(\"total_volume\"),\n",
    "                     col(\"price_percentiles\")\n",
    "                 )).alias(\"value\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Вивід у консоль для дебагінгу\n",
    "console_query = (processed_df\n",
    "                 .writeStream\n",
    "                 .trigger(processingTime=\"30 seconds\")  # Інтервал тригера\n",
    "                 .outputMode(\"update\")  # Для потокових даних\n",
    "                 .format(\"console\")\n",
    "                 .option(\"truncate\", \"false\")  # Повний вивід без обрізання\n",
    "                 .option(\"numRows\", 50)  # Кількість рядків для відображення\n",
    "                 .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Запис у Kafka\n",
    "kafka_query = (output_df\n",
    "               .writeStream\n",
    "               .trigger(processingTime=\"60 seconds\")  # Збільшено інтервал тригера для зменшення навантаження\n",
    "               .outputMode(\"update\")\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", kafka_config['bootstrap_servers'][0])\n",
    "               .option(\"topic\", 'output_topic')\n",
    "               .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\")\n",
    "               .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "               .option(\"kafka.sasl.jaas.config\",\n",
    "                       'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"VawEzo1ikLtrA8Ug8THa\";')\n",
    "               .option(\"checkpointLocation\", \"/tmp/spark_checkpoints\")\n",
    "               .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "console_query.awaitTermination()\n",
    "kafka_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
